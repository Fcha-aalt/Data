---
SCOPUS_ID: 85149400322
Title: "Fast Human-in-The-Loop Control for HVAC Systems via Meta-Learning and Model-Based Offline Reinforcement Learning"
Author: "Chen L."
Journal: "IEEE Transactions on Sustainable Computing"
Publication Date: {'$date': '2023-07-01T00:00:00Z'}
Publication Year: 2023
DOI: "10.1109/TSUSC.2023.3251302"
Source Type: "Journal"
Document Type: "ar"
Document Type Description: "Article"
Affiliation: "School of Electrical and Computer Engineering"
Affiliation Country: "United States"
Cited by count: 3
---

## Abstract
"Reinforcement learning (RL) methods can be used to develop a controller for the heating, ventilation, and air conditioning (HVAC) systems that both saves energy and ensures high occupants' thermal comfort levels. However, the existing works typically require on-policy data to train an RL agent, and the occupants' personalized thermal preferences are not considered, which is limited in the real-world scenarios. This paper designs a high-performance model-based offline RL algorithm for personalized HVAC systems. The proposed algorithm can quickly adapt to different occupants' thermal preferences with a few thermal feedbacks, guaranteeing the high occupants' personalized thermal comfort levels efficiently. First, we use a meta-supervised learning algorithm to train an occupant's thermal preference model. Then, we train an ensemble neural network to predict the thermal states of the considered zone. In addition, the obtained ensemble networks can indicate the regions in the state and action spaces covered by the offline dataset. With the personalized thermal preference model updated via meta-Testing, model-based RL is used to derive the optimal HVAC controller. Since the proposed algorithm only requires offline datasets and a few online thermal feedbacks for training, it contributes to a more practical deployment of the RL algorithm to HVAC systems. We use the ASHRAE database II to verify the effectiveness and advantage of the meta-learning algorithm for modeling different occupants' thermal preferences. Numerical simulations on the EnergyPlus environment demonstrate that the proposed algorithm can guarantee personalized thermal preferences with a slight increase of power consumption of 1.91% compared with the model-based RL algorithm with on-policy data aggregation."
